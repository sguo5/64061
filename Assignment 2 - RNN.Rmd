---
title: "Assignment 2 - RNN"
author: "Shude Guo"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r message=FALSE, warning=FALSE}
library(keras)
```

## Using an embedding layer
```{r}
embedding_layer <- layer_embedding(input_dim = 1000, output_dim = 64)
```

### 10000 words to be consider as features
```{r}
max_features <- 10000
```

### Cutoff reviews after 150 words 
```{r}
maxlen <- 150
```

### Loading the IMDB data
```{r}
imdb <- dataset_imdb(num_words = max_features)
c(c(x_train, y_train), c(x_test, y_test)) %<-% imdb
x_train <- pad_sequences(x_train, maxlen = maxlen)
x_test <- pad_sequences(x_test, maxlen = maxlen)
```

### Define the model
```{r}
model <- keras_model_sequential() %>% layer_embedding(input_dim = 10000, output_dim = 8, input_length = maxlen) %>% layer_flatten() %>% layer_dense(units = 1, activation = "sigmoid")
model %>% compile(optimizer = "rmsprop", loss = "binary_crossentropy", metrics = c("acc"))
history <- model %>% fit(x_train, y_train, epochs = 10, batch_size = 32, validation_split = 0.4)
```

### In this model, we look at 150 words from each review, and the validation accuracy that we got in this model is about 87%.

## Using pre-trained word embeddings
### Downloading the IMDB data as raw text
```{r}
setwd("~/Desktop")
imdb_dir <- "aclImdb"
train_dir <- file.path(imdb_dir, "train")
labels <- c()
texts <- c()
for (label_type in c("neg", "pos")) {
  label <- switch(label_type, neg = 0, pos = 1)
  dir_name <- file.path(train_dir, label_type)
  for (fname in list.files(dir_name, pattern = glob2rx("*.txt"), 
                           full.names = TRUE)) {
    texts <- c(texts, readChar(fname, file.info(fname)$size))
    labels <- c(labels, label)
  }
}
```

## Tokenize the data
### Cutoff reviews after 150 words
```{r}
maxlen <- 150
```

### Restrict training samples to 100
```{r}
training_samples <- 100
```

### Validate on 10,000 samples
```{r}
validation_samples <- 10000
```

### Consider only the top 10000 words
```{r}
max_words <- 10000
tokenizer <- text_tokenizer(num_words = max_words) %>% fit_text_tokenizer(texts)
sequences <- texts_to_sequences(tokenizer, texts)
word_index = tokenizer$word_index
cat("Found", length(word_index), "unique tokens.\n")
data <- pad_sequences(sequences, maxlen = maxlen)
labels <- as.array(labels)
cat("Shape of data tensor:", dim(data), "\n")
cat('Shape of label tensor:', dim(labels), "\n")
```

### Split the data into a training set and a validation set
```{r}
indices <- sample(1:nrow(data))
training_indices <- indices[1:training_samples]
validation_indices <- indices[(training_samples + 1): (training_samples + validation_samples)]
x_train <- data[training_indices,]
y_train <- labels[training_indices]
x_val <- data[validation_indices,]
y_val <- labels[validation_indices]
```

## Pre-process the embeddings
```{r}
glove_dir = 'glove'
lines <- readLines(file.path(glove_dir, "glove.6B.100d.txt"))
embeddings_index <- new.env(hash = TRUE, parent = emptyenv())
for (i in 1:length(lines)) {
  line <- lines[[i]]
  values <- strsplit(line, " ")[[1]]
  word <- values[[1]]
  embeddings_index[[word]] <- as.double(values[-1])
}
```

### Building an embedding matrix
```{r}
embedding_dim <- 100
embedding_matrix <- array(0, c(max_words, embedding_dim))
for (word in names(word_index)) {
  index <- word_index[[word]]
  if (index < max_words) {
    embedding_vector <- embeddings_index[[word]]
  if (!is.null(embedding_vector))
      embedding_matrix[index+1,] <- embedding_vector
  }
}
```

## Define the model
```{r}
model <- keras_model_sequential() %>% layer_embedding(input_dim = max_words, output_dim = embedding_dim, input_length = maxlen) %>% layer_flatten() %>% layer_dense(units = 32, activation = "relu") %>% layer_dense(units = 1, activation = "sigmoid")
summary(model)
```

### Loading the GloVe embeddings in this model
```{r}
get_layer(model, index = 1) %>% set_weights(list(embedding_matrix)) %>% freeze_weights()
```

### Training the model
```{r}
model %>% compile(optimizer = "rmsprop", loss = "binary_crossentropy", metrics = c("acc"))
history <- model %>% fit(x_train, y_train, epochs = 20, batch_size = 32, validation_data = list(x_val, y_val))
save_model_weights_hdf5(model, "pre_trained_glove_model.h5")
plot(history)
```

## Evaluate the model on the test data
```{r}
test_dir <- file.path(imdb_dir, "test")
labels <- c()
texts <- c()
for (label_type in c("neg", "pos")) {
  label <- switch(label_type, neg = 0, pos = 1)
  dir_name <- file.path(test_dir, label_type)
  for (fname in list.files(dir_name, pattern = glob2rx("*.txt"), full.names = TRUE)) {
  texts <- c(texts, readChar(fname, file.info(fname)$size))
    labels <- c(labels, label)
  }
}
sequences <- texts_to_sequences(tokenizer, texts)
x_test <- pad_sequences(sequences, maxlen = maxlen)
y_test <- as.array(labels)
model %>% load_model_weights_hdf5("pre_trained_glove_model.h5") %>% evaluate(x_test, y_test, verbose = 0)
```

### After training and evaluating the model, we got about 54% validation accuracy on this model.

## changing the number of training samples for the model, and split the data into a training set and a validation set
```{r}
training_samples <- 200
training_indices <- indices[1:training_samples]
validation_indices <- indices[(training_samples + 1): (training_samples + validation_samples)]
x_train <- data[training_indices,]
y_train <- labels[training_indices]
x_val <- data[validation_indices,]
y_val <- labels[validation_indices]
```

### Training the model with 200 sample size
```{r}
model %>% compile(optimizer = "rmsprop", loss = "binary_crossentropy", metrics = c("acc"))
history <- model %>% fit(x_train, y_train, epochs = 20, batch_size = 32, validation_data = list(x_val, y_val))
```

### The sample size has been changed from 100 to 200, the validation accuracy we got went from 54% to about 57%.
### After changing the sample size for several times, we can know that when the sample size is larger and larger, the validation accuracy we get will be higher and higher. In particular, when we set the sample size to 15,000, the validation accuracy was about 69%, the highest so far.

